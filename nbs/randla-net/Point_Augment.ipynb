{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1007800-63d8-41e3-a287-88508624a257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from imps.data.scannet import ScanNetScene, CLASS_NAMES\n",
    "from imps.sqn.model import Randla\n",
    "from imps.sqn.data_utils import prepare_input\n",
    "\n",
    "from imps.point_augment.Common import loss_utils\n",
    "from imps.point_augment.Augmentor.augmentor import Augmentor\n",
    "from imps.point_augment.Classifier.classifier import RandlaClassifier\n",
    "\n",
    "SCENE_DIR = '/app/mnt/scans/scene0040_00'\n",
    "\n",
    "N_POINTS = int(1.5e5)\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "IGNORED_LABELS = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3d1596-61a4-4cf7-9619-8b32c7983354",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = ScanNetScene(SCENE_DIR)\n",
    "\n",
    "surface_points, surface_colors, point_labels = scene.create_points_colors_labels_from_pc(N_POINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66efaccc-103f-4f77-84cb-09a44b5f0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a dictionary for storing points corresponding to the labels. I need this for extracting out table from the entire scene\n",
    "\n",
    "label_point_dict = {} # creating empty dictionary: key : label; elements for each key: surface points corresponding to each label\n",
    "for c in range(len(CLASS_NAMES)+1):\n",
    "    res_list = np.where(point_labels == c) # finding the corresponding indices for a particular label use np.where[point_labels == c]\n",
    "\n",
    "    surface_points_arr = np.array(surface_points)\n",
    "    label_point_dict[c] = (list(surface_points_arr[res_list])) # extracting surface points corresponding to the indices. Objects are separated hence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "186477d1-8be8-4b20-a351-b7d0eb661a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1416, 3])\n"
     ]
    }
   ],
   "source": [
    "label_point_dict[7] = np.array(label_point_dict[7])\n",
    "xyz_table = torch.FloatTensor(label_point_dict[7]).unsqueeze(0)\n",
    "print(xyz_table.shape)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d33e92c-4a6e-4f54-a1f1-8dc93ba179b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a dictionary for storing indexes corresponding to the labels. I need this for extracting the indices for the corresponding labels so that I can extract the labels for a particular object from the decoder output\n",
    "\n",
    "label_index_dict = {} \n",
    "for c in range(len(CLASS_NAMES)+1):\n",
    "    res_list = np.where(point_labels == c) # extracting indices corresponding to a label\n",
    "    label_index_dict[c] = res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53ae8278-e1eb-4eb3-8197-d972b117a8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed for calculation of randla_loss\n",
    "\n",
    "class_counts = []\n",
    "for c in range(len(CLASS_NAMES.keys())):\n",
    "    class_counts.append(np.sum(point_labels == c))\n",
    "class_counts = np.array(class_counts)\n",
    "\n",
    "for ign in IGNORED_LABELS:\n",
    "    class_counts[ign] = 0\n",
    "class_weights = class_counts / class_counts.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "014edebd-d478-4175-8e6b-7b21a7bb5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding out point_mask for now.\n",
    "\n",
    "def get_randla_loss(logits, labels, class_weights):\n",
    "    class_weights = torch.from_numpy(class_weights).float().to(logits.device)\n",
    "    logits = logits.reshape(-1, len(class_weights))\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights, reduction='none')\n",
    "    output_loss = criterion(logits, labels)\n",
    "    \n",
    "   \n",
    "    n_points = output_loss.shape[0]\n",
    "        \n",
    "    output_loss = output_loss.sum() / n_points\n",
    "    \n",
    "    return output_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "573add56-ed33-4520-a27c-9a7f6a534b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_aug_instance_to_pc(aug_instance, surface_points, index_list):\n",
    "    aug_instance = aug_instance.squeeze(0).transpose(1,0)\n",
    "    aug_instance = aug_instance.cpu().detach()\n",
    "    surface_points[index_list] = aug_instance\n",
    "    return surface_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a200d3b4-7658-478a-8706-17990fd6c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_label_pools(labels, input_pools, N_CLASS):\n",
    "    label_pools = []\n",
    "    prev_label_pools = labels\n",
    "\n",
    "    for ip in input_pools:\n",
    "        ip = ip.squeeze().cpu().numpy()\n",
    "        n_pts, k = ip.shape\n",
    "        ip = ip.reshape(-1)\n",
    "        pool_pt_labels = prev_label_pools[ip]\n",
    "        oh_labels = np.eye(N_CLASS)[pool_pt_labels]\n",
    "        pooled_labels = oh_labels.reshape(n_pts, k, N_CLASS)\n",
    "        pooled_votes = pooled_labels.sum(axis=1)\n",
    "\n",
    "        prev_label_pools = pooled_votes.argmax(axis=-1)\n",
    "        label_pools.append(prev_label_pools)\n",
    "        \n",
    "    return label_pools\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05c135a9-1480-4ee8-863c-ad8e9cd1de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_instance_global_features(label_pools, features_encoder_list, target_label):\n",
    "    instance_features_list = []\n",
    "\n",
    "    for index, feature in enumerate(features_encoder_list):\n",
    "        feature = feature.squeeze(0).squeeze(-1).permute(1,0)\n",
    "        labels = label_pools[index]\n",
    "        index_list = np.where(labels == target_label)\n",
    "        instance_features = (feature[index_list])\n",
    "        global_instance_features, indices =torch.max(instance_features, 0)\n",
    "        instance_features_list.append(global_instance_features)\n",
    "        global_features = torch.cat(instance_features_list,dim = 0)\n",
    "        \n",
    "    return global_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "751a1a1a-cda8-4d2f-9cd1-decad2cb7ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory used : 717MiB \n",
    "\n",
    "features = torch.FloatTensor(surface_colors).unsqueeze(0).to(DEVICE)\n",
    "xyz = torch.FloatTensor(surface_points).unsqueeze(0)\n",
    "point_labels = torch.LongTensor(point_labels).unsqueeze(0).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ab672f3-88c4-4c95-953f-0233576021ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing colors, surface_points and labels as tensors. Prepare the input for neural net operation. Initialize RandLA net\n",
    "\n",
    "# memory used: 22MiB\n",
    "\n",
    "\n",
    "\n",
    "input_points, input_neighbors, input_pools, feat_shape = prepare_input(xyz, k=16, num_layers=3, encoder_dims = [8,32,64], sub_sampling_ratio=4, \n",
    "                                                           device=DEVICE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efb7b3e6-24ec-4747-bd68-ff937e6f2b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "randla = Randla(d_feature=3, d_in=8, encoder_dims=[8, 32, 64], device=DEVICE, num_class=len(CLASS_NAMES), interpolator='keops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "861ce367-3a5d-440c-af9a-aab6291e1f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandlaClassifier(feat_shape=feat_shape, N_CLASS=len(CLASS_NAMES)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fa3e502-9fc4-4e03-8958-32fd4db4cfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz_table = xyz_table.to(DEVICE)\n",
    "xyz_table=xyz_table.transpose(2,1).contiguous()\n",
    "label_index_dict[7] = np.array(label_index_dict[7]).squeeze()\n",
    "label_index_dict[7] = label_index_dict[7].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e19ba9e5-7fbf-46bd-8b62-7430724ca73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize epoch, dimension, augmentor, optimizers\n",
    "\n",
    "# memory used: 22MiB\n",
    "\n",
    "dim = 3\n",
    "augmentor = Augmentor().cuda()\n",
    "optimizer_r = optim.Adam(randla.parameters(), lr=1e-3) # optimizer for randla \n",
    "optimizer_a = torch.optim.Adam(                        # optimzer for PA\n",
    "            augmentor.parameters(),\n",
    "            lr=0.001,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a76719-1b03-4fde-bf26-3e8f89f86ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Augmentor loss:3.5269, RandlaNet loss:0.4624, Epoch_duration: 2.07 s\n",
      "Epoch 2, Augmentor loss:3.3411, RandlaNet loss:0.432, Epoch_duration: 1.94 s\n",
      "Epoch 3, Augmentor loss:3.2831, RandlaNet loss:0.4127, Epoch_duration: 1.95 s\n",
      "Epoch 4, Augmentor loss:3.3064, RandlaNet loss:0.3985, Epoch_duration: 1.96 s\n",
      "Epoch 5, Augmentor loss:3.2021, RandlaNet loss:0.387, Epoch_duration: 1.93 s\n",
      "Epoch 6, Augmentor loss:3.1728, RandlaNet loss:0.3764, Epoch_duration: 1.93 s\n",
      "Epoch 7, Augmentor loss:3.216, RandlaNet loss:0.3669, Epoch_duration: 1.96 s\n",
      "Epoch 8, Augmentor loss:3.0239, RandlaNet loss:0.3592, Epoch_duration: 1.9 s\n",
      "Epoch 9, Augmentor loss:2.9211, RandlaNet loss:0.3531, Epoch_duration: 1.97 s\n",
      "Epoch 10, Augmentor loss:2.8224, RandlaNet loss:0.3473, Epoch_duration: 1.89 s\n",
      "Epoch 11, Augmentor loss:2.9257, RandlaNet loss:0.3425, Epoch_duration: 1.94 s\n",
      "Epoch 12, Augmentor loss:2.6091, RandlaNet loss:0.3376, Epoch_duration: 1.9 s\n",
      "Epoch 13, Augmentor loss:2.8638, RandlaNet loss:0.3323, Epoch_duration: 1.93 s\n",
      "Epoch 14, Augmentor loss:2.7984, RandlaNet loss:0.3275, Epoch_duration: 1.99 s\n",
      "Epoch 15, Augmentor loss:2.703, RandlaNet loss:0.3223, Epoch_duration: 1.9 s\n",
      "Epoch 16, Augmentor loss:2.8449, RandlaNet loss:0.3176, Epoch_duration: 1.9 s\n",
      "Epoch 17, Augmentor loss:2.6305, RandlaNet loss:0.3133, Epoch_duration: 1.9 s\n",
      "Epoch 18, Augmentor loss:2.516, RandlaNet loss:0.3097, Epoch_duration: 1.91 s\n",
      "Epoch 19, Augmentor loss:2.6532, RandlaNet loss:0.3061, Epoch_duration: 1.89 s\n",
      "Epoch 20, Augmentor loss:2.6323, RandlaNet loss:0.3024, Epoch_duration: 1.92 s\n",
      "Epoch 21, Augmentor loss:2.7111, RandlaNet loss:0.2996, Epoch_duration: 1.9 s\n",
      "Epoch 22, Augmentor loss:2.4081, RandlaNet loss:0.2967, Epoch_duration: 1.89 s\n",
      "Epoch 23, Augmentor loss:2.5327, RandlaNet loss:0.2939, Epoch_duration: 1.89 s\n",
      "Epoch 24, Augmentor loss:2.6517, RandlaNet loss:0.2909, Epoch_duration: 1.94 s\n",
      "Epoch 25, Augmentor loss:2.2707, RandlaNet loss:0.2887, Epoch_duration: 1.94 s\n",
      "Epoch 26, Augmentor loss:2.5583, RandlaNet loss:0.2863, Epoch_duration: 1.89 s\n",
      "Epoch 27, Augmentor loss:2.417, RandlaNet loss:0.2837, Epoch_duration: 1.92 s\n",
      "Epoch 28, Augmentor loss:2.4559, RandlaNet loss:0.2815, Epoch_duration: 1.88 s\n",
      "Epoch 29, Augmentor loss:2.3802, RandlaNet loss:0.2794, Epoch_duration: 1.9 s\n",
      "Epoch 30, Augmentor loss:2.2007, RandlaNet loss:0.2775, Epoch_duration: 1.9 s\n",
      "Epoch 31, Augmentor loss:2.3678, RandlaNet loss:0.2755, Epoch_duration: 1.89 s\n",
      "Epoch 32, Augmentor loss:2.2653, RandlaNet loss:0.2736, Epoch_duration: 1.9 s\n",
      "Epoch 33, Augmentor loss:2.5861, RandlaNet loss:0.2716, Epoch_duration: 1.89 s\n",
      "Epoch 34, Augmentor loss:2.2748, RandlaNet loss:0.2699, Epoch_duration: 1.91 s\n",
      "Epoch 35, Augmentor loss:2.3168, RandlaNet loss:0.2682, Epoch_duration: 1.97 s\n",
      "Epoch 36, Augmentor loss:2.101, RandlaNet loss:0.2667, Epoch_duration: 1.92 s\n",
      "Epoch 37, Augmentor loss:2.0382, RandlaNet loss:0.2649, Epoch_duration: 1.98 s\n",
      "Epoch 38, Augmentor loss:2.2804, RandlaNet loss:0.2634, Epoch_duration: 1.91 s\n",
      "Epoch 39, Augmentor loss:2.0415, RandlaNet loss:0.2617, Epoch_duration: 1.89 s\n",
      "Epoch 40, Augmentor loss:2.2011, RandlaNet loss:0.2601, Epoch_duration: 1.96 s\n",
      "Epoch 41, Augmentor loss:2.6289, RandlaNet loss:0.2584, Epoch_duration: 1.99 s\n",
      "Epoch 42, Augmentor loss:2.054, RandlaNet loss:0.2565, Epoch_duration: 1.95 s\n",
      "Epoch 43, Augmentor loss:2.0492, RandlaNet loss:0.2549, Epoch_duration: 1.9 s\n",
      "Epoch 44, Augmentor loss:2.0791, RandlaNet loss:0.2534, Epoch_duration: 1.89 s\n",
      "Epoch 45, Augmentor loss:2.1817, RandlaNet loss:0.252, Epoch_duration: 1.99 s\n",
      "Epoch 46, Augmentor loss:2.4373, RandlaNet loss:0.2506, Epoch_duration: 1.95 s\n",
      "Epoch 47, Augmentor loss:2.2314, RandlaNet loss:0.2491, Epoch_duration: 1.93 s\n",
      "Epoch 48, Augmentor loss:2.1647, RandlaNet loss:0.2477, Epoch_duration: 1.98 s\n",
      "Epoch 49, Augmentor loss:2.1628, RandlaNet loss:0.2463, Epoch_duration: 1.98 s\n",
      "Epoch 50, Augmentor loss:2.3513, RandlaNet loss:0.2448, Epoch_duration: 1.99 s\n",
      "Epoch 51, Augmentor loss:2.4908, RandlaNet loss:0.2433, Epoch_duration: 1.92 s\n",
      "Epoch 52, Augmentor loss:2.1108, RandlaNet loss:0.242, Epoch_duration: 1.94 s\n",
      "Epoch 53, Augmentor loss:2.3307, RandlaNet loss:0.2406, Epoch_duration: 1.91 s\n",
      "Epoch 54, Augmentor loss:2.0665, RandlaNet loss:0.2393, Epoch_duration: 1.92 s\n",
      "Epoch 55, Augmentor loss:2.6079, RandlaNet loss:0.2381, Epoch_duration: 1.89 s\n",
      "Epoch 56, Augmentor loss:2.534, RandlaNet loss:0.2368, Epoch_duration: 1.91 s\n",
      "Epoch 57, Augmentor loss:2.1872, RandlaNet loss:0.2355, Epoch_duration: 1.9 s\n",
      "Epoch 58, Augmentor loss:2.1062, RandlaNet loss:0.2341, Epoch_duration: 1.9 s\n",
      "Epoch 59, Augmentor loss:2.1344, RandlaNet loss:0.2331, Epoch_duration: 1.91 s\n",
      "Epoch 60, Augmentor loss:2.0734, RandlaNet loss:0.2317, Epoch_duration: 1.94 s\n",
      "Epoch 61, Augmentor loss:2.3282, RandlaNet loss:0.2306, Epoch_duration: 1.91 s\n",
      "Epoch 62, Augmentor loss:2.0601, RandlaNet loss:0.2292, Epoch_duration: 1.91 s\n",
      "Epoch 63, Augmentor loss:2.0462, RandlaNet loss:0.2281, Epoch_duration: 1.94 s\n",
      "Epoch 64, Augmentor loss:2.3412, RandlaNet loss:0.2268, Epoch_duration: 1.95 s\n",
      "Epoch 65, Augmentor loss:2.6219, RandlaNet loss:0.2257, Epoch_duration: 1.97 s\n",
      "Epoch 66, Augmentor loss:2.1809, RandlaNet loss:0.2244, Epoch_duration: 1.91 s\n",
      "Epoch 67, Augmentor loss:2.2332, RandlaNet loss:0.2233, Epoch_duration: 1.9 s\n",
      "Epoch 68, Augmentor loss:1.9975, RandlaNet loss:0.2222, Epoch_duration: 1.96 s\n",
      "Epoch 69, Augmentor loss:1.9898, RandlaNet loss:0.2212, Epoch_duration: 2.02 s\n",
      "Epoch 70, Augmentor loss:2.0093, RandlaNet loss:0.2203, Epoch_duration: 1.97 s\n",
      "Epoch 71, Augmentor loss:2.2512, RandlaNet loss:0.2192, Epoch_duration: 1.96 s\n",
      "Epoch 72, Augmentor loss:2.1161, RandlaNet loss:0.2182, Epoch_duration: 1.9 s\n",
      "Epoch 73, Augmentor loss:2.454, RandlaNet loss:0.2171, Epoch_duration: 1.91 s\n",
      "Epoch 74, Augmentor loss:2.2067, RandlaNet loss:0.216, Epoch_duration: 1.97 s\n",
      "Epoch 75, Augmentor loss:2.1769, RandlaNet loss:0.2147, Epoch_duration: 1.89 s\n",
      "Epoch 76, Augmentor loss:2.4592, RandlaNet loss:0.2134, Epoch_duration: 1.98 s\n",
      "Epoch 77, Augmentor loss:1.9921, RandlaNet loss:0.2121, Epoch_duration: 1.91 s\n",
      "Epoch 78, Augmentor loss:2.0095, RandlaNet loss:0.2108, Epoch_duration: 1.95 s\n",
      "Epoch 79, Augmentor loss:2.1005, RandlaNet loss:0.2096, Epoch_duration: 1.93 s\n",
      "Epoch 80, Augmentor loss:2.6315, RandlaNet loss:0.2085, Epoch_duration: 1.89 s\n",
      "Epoch 81, Augmentor loss:2.1459, RandlaNet loss:0.2074, Epoch_duration: 1.89 s\n",
      "Epoch 82, Augmentor loss:2.447, RandlaNet loss:0.2063, Epoch_duration: 1.93 s\n",
      "Epoch 83, Augmentor loss:2.072, RandlaNet loss:0.2053, Epoch_duration: 1.97 s\n",
      "Epoch 84, Augmentor loss:2.0554, RandlaNet loss:0.2042, Epoch_duration: 1.94 s\n",
      "Epoch 85, Augmentor loss:2.1773, RandlaNet loss:0.2032, Epoch_duration: 1.92 s\n",
      "Epoch 86, Augmentor loss:2.2552, RandlaNet loss:0.2021, Epoch_duration: 1.89 s\n",
      "Epoch 87, Augmentor loss:2.1646, RandlaNet loss:0.2009, Epoch_duration: 1.92 s\n",
      "Epoch 88, Augmentor loss:2.2087, RandlaNet loss:0.1997, Epoch_duration: 1.96 s\n",
      "Epoch 89, Augmentor loss:2.099, RandlaNet loss:0.1986, Epoch_duration: 1.9 s\n",
      "Epoch 90, Augmentor loss:2.0751, RandlaNet loss:0.1975, Epoch_duration: 1.9 s\n",
      "Epoch 91, Augmentor loss:2.1194, RandlaNet loss:0.1963, Epoch_duration: 1.94 s\n",
      "Epoch 92, Augmentor loss:2.2797, RandlaNet loss:0.1952, Epoch_duration: 1.9 s\n",
      "Epoch 93, Augmentor loss:2.1037, RandlaNet loss:0.1942, Epoch_duration: 1.9 s\n",
      "Epoch 94, Augmentor loss:2.0037, RandlaNet loss:0.193, Epoch_duration: 1.95 s\n",
      "Epoch 95, Augmentor loss:2.0035, RandlaNet loss:0.192, Epoch_duration: 1.96 s\n",
      "Epoch 96, Augmentor loss:2.0023, RandlaNet loss:0.1908, Epoch_duration: 1.89 s\n",
      "Epoch 97, Augmentor loss:2.0194, RandlaNet loss:0.1898, Epoch_duration: 1.9 s\n",
      "Epoch 98, Augmentor loss:1.9998, RandlaNet loss:0.1887, Epoch_duration: 1.93 s\n",
      "Epoch 99, Augmentor loss:2.2116, RandlaNet loss:0.1878, Epoch_duration: 1.87 s\n",
      "Epoch 100, Augmentor loss:2.0937, RandlaNet loss:0.1868, Epoch_duration: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "randla.train()\n",
    "augmentor=augmentor.train()\n",
    "\n",
    "for epoch in range(0, 100):  \n",
    "    # Original Point Cloud operation: \n",
    "    start_step1 = time.time()\n",
    "    features_encoder_list = randla.encoder(features, input_points, input_neighbors, input_pools)\n",
    "    label_pools = collect_label_pools(point_labels.squeeze().cpu().numpy(), input_pools, N_CLASS=len(CLASS_NAMES))\n",
    "    global_features = collect_instance_global_features(label_pools, features_encoder_list, target_label=7)\n",
    "    table_logits = classifier.forward(global_features.unsqueeze(0))\n",
    "\n",
    "    # Noise variable added to generate feature matrix \n",
    "    \n",
    "    noise = 0.02 * torch.randn(1, 1024).cuda() # choose randomly between 1 and 1024 for now. Here they have settled the feature array to have a dimension of 1024. We can change that?\n",
    "    optimizer_a.zero_grad()\n",
    "    optimizer_r.zero_grad()\n",
    "    # Augment the table:\n",
    "    aug_table = augmentor(xyz_table, noise) # augmented the table; #removed the batch-normal ??? \n",
    "    # here were logits \n",
    "    # Generating the modified point cloud by putting back the augmented table into it\n",
    "    augmented_scene = add_aug_instance_to_pc(aug_table, surface_points, label_index_dict[7]) #function can be improved later\n",
    "    augmented_scene = torch.FloatTensor(augmented_scene).unsqueeze(0)\n",
    "    \n",
    "    # Augmented Point Cloud Operation:\n",
    "    \n",
    "    # DOUBT:# I have to do this. As augmented point cloud is generated during each epoch and I need to prepare input for it. Any solution?\n",
    "    input_points_aug, input_neighbors_aug, input_pools_aug, feat_shape = prepare_input(augmented_scene, k=16, num_layers=3, sub_sampling_ratio=4, \n",
    "                                                           device=DEVICE)  \n",
    "    \n",
    "    \n",
    "    features_encoder_list_aug = randla.encoder(features, input_points_aug, input_neighbors_aug, input_pools_aug)\n",
    "    label_pools_aug = collect_label_pools(point_labels.squeeze().cpu().numpy(), input_pools_aug, N_CLASS=len(CLASS_NAMES))\n",
    "    global_features_aug = collect_instance_global_features(label_pools_aug, features_encoder_list_aug, target_label=7)\n",
    "    aug_table_logits = classifier.forward(global_features_aug.unsqueeze(0))\n",
    "    \n",
    " \n",
    "    logits_aug = randla.decoder(features_encoder_list_aug, input_points_aug)   \n",
    "    \n",
    "    augLoss  = loss_utils.aug_loss(table_logits, aug_table_logits, 7)  \n",
    "    randlaLoss = get_randla_loss(logits_aug, point_labels, class_weights) #how do I calculate randla_loss here? Do I calculate loss between logits_augmented and logits true?\n",
    "    \n",
    "    augLoss.backward(retain_graph=True)\n",
    "    randlaLoss.backward(retain_graph=True)\n",
    "\n",
    "    optimizer_r.step()\n",
    "    optimizer_a.step()\n",
    "    start_step2 = time.time()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Augmentor loss:{round(augLoss.item(), 4)}, RandlaNet loss:{round(randlaLoss.item(), 4)}, Epoch_duration: {round((start_step2-start_step1),2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15339951-6cd6-4591-8b0f-3a2ec1fa290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(randla.state_dict(), '../../processed/saved_models/PA_scene0040_00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1727228f-5dfb-45c2-918e-c8581add113d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rotating, scaling and translating a table individually and putting it back to the original point cloud ###\n",
    "\n",
    "\n",
    "surface_points_table = surface_points_table_[np.newaxis, :, :]\n",
    "point_cloud_rotate = PointcloudRotatebyAngle(surface_points_table)\n",
    "\n",
    "print(surface_points_table.shape)\n",
    "surface_points_table = torch.from_numpy(surface_points_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b506c-fbfb-4c92-a5b7-8b22064d0291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmentation: Rotation\n",
    "\n",
    "point_cloud_table_rotated = point_cloud_rotate(surface_points_table, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdd38b7-07ed-476d-8999-1577bb6aee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation: Scale and Translate\n",
    "\n",
    "point_cloud_scale_and_translate = PointcloudScaleAndTranslate()\n",
    "point_cloud_table_st = point_cloud_scale_and_translate(point_cloud_table_rotated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3892d019-832f-4d7b-ab2b-2caae66b8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding augmented point cloud back to the original point cloud\n",
    "\n",
    "point_cloud_table_st = point_cloud_table_st.cpu()\n",
    "surface_points1[res_list] = point_cloud_table_st\n",
    "\n",
    "print(surface_points1.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
