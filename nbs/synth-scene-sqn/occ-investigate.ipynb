{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, '/home/cem/Documents/imps/src')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import jaccard_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import cm\n",
    "\n",
    "from imps.sqn.model import SQN\n",
    "from imps.sqn.data_utils import prepare_input\n",
    "from imps.metrics import compute_iou\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "DATA_ROOT = '/mnt/data.nas/shareddata/6G-futurelab/synthetic_room_dataset/rooms_04'\n",
    "scene = '00000004'\n",
    "POS_EMBEDDING = True\n",
    "MODEL_DIR = './logs/occ-semantic-emb-skip-seed=31-lr=sch-1638545527/model'\n",
    "\n",
    "scene_dir = os.path.join(DATA_ROOT, scene)\n",
    "\n",
    "# Classes taken from: https://github.com/autonomousvision/convolutional_occupancy_networks/blob/master/scripts/dataset_synthetic_room/build_dataset.py#L29\n",
    "# https://gist.github.com/tejaskhot/15ae62827d6e43b91a4b0c5c850c168e\n",
    "classes = ['04256520', '03636649', '03001627', '04379243', '02933112']\n",
    "class_names = ['sofa', 'lamp', 'chair', 'table', 'cabinet']\n",
    "classes, class_names = zip(*sorted(zip(classes, class_names)))\n",
    "class2name = {\n",
    "    -1: \"ground-plane\",\n",
    "    6: \"free-space\"\n",
    "}\n",
    "for i in range(len(class_names)):\n",
    "    class2name[i] = class_names[i]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "    \n",
    "def get_loss(logits, labels, pos_weight):\n",
    "    n_batch = logits.shape[0]\n",
    "    logits = logits.reshape(n_batch, -1)\n",
    "    labels = labels.reshape(n_batch, -1)\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='none')\n",
    "    output_loss = criterion(logits, labels)\n",
    "    output_loss = output_loss.mean()\n",
    "    \n",
    "    return output_loss\n",
    "\n",
    "def get_semantic_iou(logits, labels, n_class):\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    ious = []\n",
    "    \n",
    "    for c in range(n_class):\n",
    "        iou = jaccard_score((labels==c).astype(int), (preds==c).astype(int), pos_label=1)\n",
    "        ious.append(iou)\n",
    "        \n",
    "    return np.array(ious)\n",
    "\n",
    "def pos_embed(pos, L=10):\n",
    "    embs = []\n",
    "    \n",
    "    for l in range(L):\n",
    "        sin_emb = np.sin((2^l)*np.pi*pos)\n",
    "        cos_emb = np.cos((2^l)*np.pi*pos)\n",
    "        embs += [sin_emb, cos_emb]\n",
    "    \n",
    "    return np.concatenate(embs, axis=-1).astype(np.float)\n",
    "\n",
    "def get_data(scene_points, query_points, query_occ, embedding=False, seed=None):\n",
    "    \n",
    "    if embedding:\n",
    "        features = torch.FloatTensor(pos_embed(scene_points)).unsqueeze(0).to(DEVICE)\n",
    "    else:\n",
    "        features = torch.FloatTensor(scene_points).unsqueeze(0).to(DEVICE)\n",
    "        \n",
    "    xyz = torch.FloatTensor(scene_points).unsqueeze(0)\n",
    "    \n",
    "    # This should be permutation invariant but it is not! WHY!!!\n",
    "    # Hypothesis: When we permute and sub-sample, during the kNN up-sampling part, the\n",
    "    # corresponding features will change.\n",
    "    # We have to permute the input since it is ordered wrt. to objects.\n",
    "    \n",
    "    if (seed is not None) and (seed != -1):\n",
    "        torch.manual_seed(seed)\n",
    "    elif seed == -1:\n",
    "        torch.manual_seed(31)\n",
    "    \n",
    "    point_perm = torch.randperm(xyz.size()[1])\n",
    "    xyz = xyz[:, point_perm]\n",
    "    features = features[:, point_perm]\n",
    "\n",
    "    query = torch.FloatTensor(query_points).unsqueeze(0).to(DEVICE)\n",
    "    query_labels = torch.FloatTensor(query_occ).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    input_points, input_neighbors, input_pools = prepare_input(xyz, k=8, num_layers=3, sub_sampling_ratio=4, \n",
    "                                                           device=DEVICE)\n",
    "    \n",
    "    return features, input_points, input_neighbors, input_pools, query, query_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91507cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_pcd = o3d.io.read_point_cloud(os.path.join(scene_dir, 'pointcloud0.ply'));\n",
    "\n",
    "scene = np.load(os.path.join(scene_dir, 'pointcloud', 'pointcloud_00.npz'))\n",
    "query_iou = np.load(os.path.join(scene_dir, 'points_iou', 'points_iou_00.npz'))\n",
    "\n",
    "scene_points = scene['points']\n",
    "query_points = query_iou['points']\n",
    "query_occ = np.unpackbits(query_iou['occupancies'])\n",
    "query_semantics = query_iou['semantics']\n",
    "\n",
    "pos_w = np.sum(query_occ==0) / np.sum(query_occ==1)\n",
    "pos_w = torch.FloatTensor([pos_w]).to(DEVICE)\n",
    "\n",
    "query_pcd = o3d.geometry.PointCloud()\n",
    "query_pcd.points = o3d.utility.Vector3dVector(query_points[query_occ==1])\n",
    "\n",
    "sqn = SQN(d_feature=60, d_in=64, encoder_dims=[32, 64, 128], decoder_dims=[128, 32, 1], device=DEVICE, \n",
    "          skip_connections=True, second_head=5)\n",
    "\n",
    "features, input_points, input_neighbors, input_pools, query, query_labels = get_data(scene_points, \n",
    "                                                                                     query_points,\n",
    "                                                                                     query_occ,\n",
    "                                                                                     embedding=POS_EMBEDDING,\n",
    "                                                                                     seed=31)\n",
    "\n",
    "sqn.load_state_dict(torch.load(MODEL_DIR));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd76b17c",
   "metadata": {},
   "source": [
    "### Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad8629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sqn.eval()\n",
    "    logits, _ = sqn.forward(features, input_points, input_neighbors, input_pools, query)\n",
    "    pred = torch.nn.Sigmoid()(logits)\n",
    "\n",
    "    pred = logits.detach().cpu().numpy().squeeze()\n",
    "    pred = (pred > 0.5).astype(np.int)\n",
    "    gold = query_labels.detach().cpu().numpy().squeeze()\n",
    "    \n",
    "    print(jaccard_score(gold, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f804a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_pts = query_points[pred == 1]\n",
    "\n",
    "occ_pcd = o3d.geometry.PointCloud()\n",
    "occ_pcd.points = o3d.utility.Vector3dVector(occ_pts)\n",
    "\n",
    "o3d.visualization.draw_geometries([occ_pcd])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4961584d",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sqn.eval()\n",
    "    encoder_list = sqn.encoder(features, input_points, input_neighbors, input_pools)\n",
    "    layer_latents = sqn.get_features(query, encoder_list, input_points)\n",
    "    \n",
    "    latent = torch.cat(layer_latents, dim=-1).squeeze().detach().cpu().numpy()\n",
    "    layer_latents = [x.squeeze().detach().cpu().numpy() for x in layer_latents]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3553a1",
   "metadata": {},
   "source": [
    "### Extract scene-specific latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ed87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = latent\n",
    "\n",
    "pca = PCA(2).fit(f)\n",
    "latent_pc = pca.transform(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64483a62",
   "metadata": {},
   "source": [
    "### Plot semantics (one-shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214ae7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11 ,11))\n",
    "# Ignore wall, plane and free-space\n",
    "ignore = (-1, 6)\n",
    "\n",
    "for i in np.unique(query_semantics):\n",
    "    if i not in ignore:\n",
    "        mask = query_semantics == i\n",
    "        plt.scatter(latent_pc[mask, 0], latent_pc[mask, 1], label=class2name[i])\n",
    "    \n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3311e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imps.utils.libmise import MISE\n",
    "\n",
    "resolution0 = 16\n",
    "upsampling_steps = 3\n",
    "padding = 0.1\n",
    "box_size = 1 + padding\n",
    "threshold = 0.99\n",
    "\n",
    "mesh_extractor = MISE(resolution0, upsampling_steps, threshold)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sqn.eval()\n",
    "    encoder_list = sqn.encoder(features, input_points, input_neighbors, input_pools)\n",
    "\n",
    "eval_points = []\n",
    "eval_values = []\n",
    "\n",
    "points = mesh_extractor.query()\n",
    "while points.shape[0] != 0:\n",
    "    # Query points\n",
    "    pointsf = points / mesh_extractor.resolution\n",
    "    # Normalize to bounding box\n",
    "    pointsf = box_size * (pointsf - 0.5)\n",
    "    eval_points.append(pointsf)\n",
    "    pointsf = torch.FloatTensor(pointsf).to(DEVICE).unsqueeze(0)\n",
    "    # Evaluate model and update\n",
    "    # values = self.eval_points(pointsf, c, **kwargs).cpu().numpy()\n",
    "    with torch.no_grad():\n",
    "        values, _ = sqn.decoder(encoder_list, input_points, pointsf)\n",
    "        # values = torch.nn.Sigmoid()(values)\n",
    "        values = values.squeeze().cpu().numpy()\n",
    "        eval_values.append(sigmoid(values))\n",
    "        \n",
    "    values = values.astype(np.float64)\n",
    "    mesh_extractor.update(points, values)\n",
    "    points = mesh_extractor.query()\n",
    "\n",
    "value_grid = mesh_extractor.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46be002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does not work yet since it is so overfitted to the points, when we query points around the scene,\n",
    "# it fails to predict the points.\n",
    "cmap = cm.get_cmap('Reds')\n",
    "idx = 5\n",
    "\n",
    "eval_pcd = o3d.geometry.PointCloud()\n",
    "eval_pcd.points = o3d.utility.Vector3dVector(eval_points[idx])\n",
    "eval_pcd.colors = o3d.utility.Vector3dVector(cmap(eval_values[idx])[:, :-1])\n",
    "\n",
    "o3d.visualization.draw_geometries([occ_pcd, eval_pcd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71934397",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mcubes\n",
    "import trimesh\n",
    "\n",
    "vertices, triangles = mcubes.marching_cubes(sigmoid(value_grid), threshold)\n",
    "mesh = trimesh.Trimesh(vertices, triangles)\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f824f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3d-env",
   "language": "python",
   "name": "3d-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
